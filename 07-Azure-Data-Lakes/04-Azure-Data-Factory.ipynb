{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c50c8fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>\n",
    "\n",
    "# Azure Data Factory with Python\n",
    "\n",
    "## Azure Actions Covered\n",
    "\n",
    "* Creating and listing Azure Data Factories\n",
    "* Linking resources to an Azure Data Factory\n",
    "* Linking datasets to an Azure Data Factory\n",
    "* Setting up and running pipelines\n",
    "\n",
    "In this lecture, we'll learn how to set up Azure Data Factories and data pipelines with Python.\n",
    "\n",
    "To begin, we'll need to import our usual libraries as well as any useful environment variables (e.g. AZURE_SUBSCRIPTION_ID). We'll add some new imports as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2873178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "# New imports for data lake storage\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "from azure.mgmt.datafactory import models \n",
    "\n",
    "from settings import AZURE_SUBSCRIPTION_ID, DEFAULT_LOCATION, DEFAULT_RESOURCE_GROUP, DATA_LAKE_CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9f118",
   "metadata": {},
   "source": [
    "Let's instantiate our credential and a `DataFactoryManagementClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff128d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = AzureCliCredential()\n",
    "df_client = DataFactoryManagementClient(credential, AZURE_SUBSCRIPTION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cd95b",
   "metadata": {},
   "source": [
    "## Data Factory\n",
    "\n",
    "We can create a new Azure Data Factory with these parameters:\n",
    "* `resource_group_name` - Name of resource group under which to create ADF\n",
    "* `factory_name` - Name for new ADF\n",
    "* `factory` - Parameters for new ADF, based on `Factory` object. For full list of parameters, see the [Factory class](https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.factory?view=azure-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22fbe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory = df_client.factories.create_or_update(\n",
    "    resource_group_name=DEFAULT_RESOURCE_GROUP,\n",
    "    factory_name='bens-data-factory1234',\n",
    "    factory=models.Factory(location=DEFAULT_LOCATION)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2752af",
   "metadata": {},
   "source": [
    "## Linked Services\n",
    "\n",
    "In ADF, you can link to the various resources you might need to process data (e.g. data storage). All of these resources have their own models. Some examples are:\n",
    "* `AzureStorageLinkedService`\n",
    "* `AzureBlobStorageLinkedService`\n",
    "* `FtpServerLinkedService`\n",
    "\n",
    "For the full list of linked service models, see [the LinkedService base class](https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.linkedservice?view=azure-python)\n",
    "\n",
    "We'll connected to our data lake storage. First, we'll create the linked service resource object in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8228d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dl_store = models.LinkedServiceResource(\n",
    "    properties=models.AzureStorageLinkedService(\n",
    "        connection_string=DATA_LAKE_CONNECTION_STRING\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab0472",
   "metadata": {},
   "source": [
    "Next, we'll use our data factory client to create the linked service in our factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f10fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = df_client.linked_services.create_or_update(\n",
    "    resource_group_name=DEFAULT_RESOURCE_GROUP,\n",
    "    factory_name='bens-data-factory1234',\n",
    "    linked_service_name='bens-storage-linked-service-1',\n",
    "    linked_service=ls_dl_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9057809",
   "metadata": {},
   "source": [
    "Let's look at some of the attributes of our returned linked service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba402377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Microsoft.DataFactory/factories/linkedservices'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fbf73da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bens-data-lake-linked-service-1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44e1b7",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "To work with datasets in ADF, we need to create them just like the linked resouces. Additionally, the datasets should be tied to our linked services. The pipeline we're going to create will copy data from one data source to another, so we'll need two datasets.\n",
    "\n",
    "The `create_or_update()` method for datasets takes the following parameters:\n",
    "* `resource_group_name` - Name for the resource group\n",
    "* `factory_name` - Name of the factory for the dataset\n",
    "* `dataset_name` - Name for our dataset\n",
    "* `dataset` - Properties for the dataset. These can be represented by a `DatasetResource` object, whose properties will be a `Dataset` object. For a list of all possible dataset classes, see [the Dataset base class](https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.dataset?view=azure-python). Some examples are:\n",
    "    * `AzureBlobDataset`\n",
    "    * `AzurePostgreSqlTableDataset`\n",
    "    * `SnowflakeDataset`\n",
    "\n",
    "We'll first create our input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa98cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_resource = df_client.datasets.create_or_update(\n",
    "    resource_group_name=DEFAULT_RESOURCE_GROUP,\n",
    "    factory_name='bens-data-factory1234',\n",
    "    dataset_name='income',\n",
    "    dataset=models.DatasetResource(\n",
    "        properties=models.AzureBlobDataset(\n",
    "            linked_service_name=models.LinkedServiceReference(\n",
    "                type='LinkedServiceReference',\n",
    "                reference_name=ls.name\n",
    "            ),\n",
    "            folder_path='dl-file-system/raw-data',\n",
    "            file_name='income.csv'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ddc8f",
   "metadata": {},
   "source": [
    "Let's look at some of the returned properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4517010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/bf8c33be-e4bb-46c8-871a-85182d913c50/resourceGroups/default-resource-group/providers/Microsoft.DataFactory/factories/bens-data-factory1234/datasets/income',\n",
       " 'name': 'income',\n",
       " 'type': 'Microsoft.DataFactory/factories/datasets',\n",
       " 'etag': '75008a5e-0000-0100-0000-648229060000',\n",
       " 'properties': {'type': 'AzureBlob',\n",
       "  'linked_service_name': {'type': 'LinkedServiceReference',\n",
       "   'reference_name': 'bens-storage-linked-service'},\n",
       "  'folder_path': 'dl-file-system/raw-data',\n",
       "  'file_name': 'income.csv'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_resource.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93515fe0",
   "metadata": {},
   "source": [
    "Now let's create the output dataset in our ADF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51a2f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out_resource = df_client.datasets.create_or_update(\n",
    "    resource_group_name=DEFAULT_RESOURCE_GROUP,\n",
    "    factory_name=data_factory.name,\n",
    "    dataset_name='income_transformed',\n",
    "    dataset=DatasetResource(\n",
    "        properties=AzureBlobDataset(\n",
    "            linked_service_name=LinkedServiceReference(\n",
    "                type='LinkedServiceReference',\n",
    "                reference_name=ls.name\n",
    "            ),\n",
    "            folder_path='dl-file-system/raw-data',\n",
    "            file_name='income_transformed.csv'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b26c3",
   "metadata": {},
   "source": [
    "## Activities, Pipelines, and Pipeline Runs\n",
    "\n",
    "A pipeline in ADF will consist of one or more activities, so first we'll need to create the activity. In this lecture, we'll have a simple copy activity. The copy activity needs:\n",
    "* Inputs/sources\n",
    "* Outputs/sinks\n",
    "\n",
    "These will come from our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df6a0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_activity = CopyActivity(\n",
    "    name='copyIncomeData',\n",
    "    inputs=[DatasetReference(type=\"DatasetReference\", reference_name=ds_resource.name)],\n",
    "    outputs=[DatasetReference(type=\"DatasetReference\", reference_name=ds_out_resource.name)],\n",
    "    source=BlobSource(),\n",
    "    sink=BlobSink()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c609c",
   "metadata": {},
   "source": [
    "Now that we've defined our activity, we can create a new pipeline in our data factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1fcaf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = df_client.pipelines.create_or_update(\n",
    "    resource_group_name=DEFAULT_RESOURCE_GROUP,\n",
    "    factory_name=data_factory.name,\n",
    "    pipeline_name='bens-pipeline',\n",
    "    pipeline=PipelineResource(\n",
    "        activities=[copy_activity],\n",
    "        parameters={}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f7ead",
   "metadata": {},
   "source": [
    "Once it's created, let's run our new pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24aa407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = df_client.pipelines.create_run(\n",
    "    resource_group_name=DEFAULT_RESOURCE_GROUP,\n",
    "    factory_name=data_factory.name,\n",
    "    pipeline_name='bens-pipeline',\n",
    "    parameters={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dd0bb",
   "metadata": {},
   "source": [
    "To manage our pipelines, we can list all of them by data factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "025055d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': None, 'id': '/subscriptions/bf8c33be-e4bb-46c8-871a-85182d913c50/resourceGroups/default-resource-group/providers/Microsoft.DataFactory/factories/bens-data-factory1234/pipelines/bens-pipeline', 'name': 'bens-pipeline', 'type': 'Microsoft.DataFactory/factories/pipelines', 'etag': '75001a5f-0000-0100-0000-64822c760000', 'description': None, 'activities': [<azure.mgmt.datafactory.models._models_py3.CopyActivity object at 0x7f3f35775730>], 'parameters': {}, 'variables': None, 'concurrency': None, 'annotations': None, 'run_dimensions': None, 'folder': None, 'policy': None}\n",
      "{'additional_properties': None, 'id': '/subscriptions/bf8c33be-e4bb-46c8-871a-85182d913c50/resourceGroups/default-resource-group/providers/Microsoft.DataFactory/factories/bens-data-factory1234/pipelines/test-pipeline', 'name': 'test-pipeline', 'type': 'Microsoft.DataFactory/factories/pipelines', 'etag': '75003a64-0000-0100-0000-6482500c0000', 'description': None, 'activities': [<azure.mgmt.datafactory.models._models_py3.ExecuteDataFlowActivity object at 0x7f3f357759d0>], 'parameters': None, 'variables': None, 'concurrency': None, 'annotations': [], 'run_dimensions': None, 'folder': None, 'policy': <azure.mgmt.datafactory.models._models_py3.PipelinePolicy object at 0x7f3f35775940>}\n"
     ]
    }
   ],
   "source": [
    "for pipe in df_client.pipelines.list_by_factory(DEFAULT_RESOURCE_GROUP, data_factory.name):\n",
    "    print(pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
